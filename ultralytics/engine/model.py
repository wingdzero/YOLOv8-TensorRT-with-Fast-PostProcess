# Ultralytics YOLO ðŸš€, AGPL-3.0 license

import inspect
from pathlib import Path
from typing import List, Union

import numpy as np
import torch

from ultralytics.cfg import TASK2DATA, get_cfg, get_save_dir
from ultralytics.engine.results import Results
from ultralytics.hub import HUB_WEB_ROOT, HUBTrainingSession
from ultralytics.nn.tasks import attempt_load_one_weight, guess_model_task, nn, yaml_model_load
from ultralytics.utils import (
    ARGV,
    ASSETS,
    DEFAULT_CFG_DICT,
    LOGGER,
    RANK,
    callbacks,
    checks,
    emojis,
    yaml_load,
)


class Model(nn.Module):
    """
        ä¸€ä¸ªç”¨äºŽå®žçŽ° YOLO æ¨¡åž‹çš„åŸºç±»ï¼Œç»Ÿä¸€ä¸åŒæ¨¡åž‹ç±»åž‹çš„ API
        è¯¥ç±»ä¸º YOLO æ¨¡åž‹ç›¸å…³çš„å„ç§æ“ä½œï¼ˆå¦‚è®­ç»ƒã€éªŒè¯ã€é¢„æµ‹ã€å¯¼å‡ºå’ŒåŸºå‡†æµ‹è¯•ï¼‰æä¾›äº†ä¸€ä¸ªé€šç”¨æŽ¥å£ã€‚å®ƒå¤„ç†ä¸åŒç±»åž‹çš„æ¨¡åž‹ï¼Œ
        åŒ…æ‹¬ä»Žæœ¬åœ°æ–‡ä»¶ã€Ultralytics HUB æˆ– Triton Server åŠ è½½çš„æ¨¡åž‹ã€‚è¯¥ç±»è®¾è®¡çµæ´»ä¸”å¯æ‰©å±•ï¼Œé€‚ç”¨äºŽä¸åŒçš„ä»»åŠ¡å’Œæ¨¡åž‹é…ç½®

        å‚æ•°ï¼š
            modelï¼ˆUnion[str, Path], å¯é€‰ï¼‰ï¼šè¦åŠ è½½æˆ–åˆ›å»ºçš„æ¨¡åž‹è·¯å¾„æˆ–åç§°ã€‚å¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€æ¥è‡ª Ultralytics HUB çš„æ¨¡åž‹åç§°æˆ– Triton Server æ¨¡åž‹ã€‚é»˜è®¤ä¸º 'yolov8n.pt'
            taskï¼ˆAny, å¯é€‰ï¼‰ï¼šä¸Ž YOLO æ¨¡åž‹å…³è”çš„ä»»åŠ¡ç±»åž‹ã€‚å¯ä»¥ç”¨äºŽæŒ‡å®šæ¨¡åž‹çš„åº”ç”¨é¢†åŸŸï¼Œå¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ç­‰ã€‚é»˜è®¤ä¸º None
            verboseï¼ˆbool, å¯é€‰ï¼‰ï¼šå¦‚æžœä¸º Trueï¼Œåˆ™åœ¨æ¨¡åž‹æ“ä½œæœŸé—´å¯ç”¨è¯¦ç»†è¾“å‡ºã€‚é»˜è®¤ä¸º False

        å±žæ€§ï¼š
            callbacksï¼ˆdictï¼‰ï¼šåœ¨æ¨¡åž‹æ“ä½œæœŸé—´ç”¨äºŽå„ç§äº‹ä»¶çš„å›žè°ƒå‡½æ•°å­—å…¸
            predictorï¼ˆBasePredictorï¼‰ï¼šç”¨äºŽè¿›è¡Œé¢„æµ‹çš„é¢„æµ‹å™¨å¯¹è±¡
            modelï¼ˆnn.Moduleï¼‰ï¼šåŸºç¡€çš„ PyTorch æ¨¡åž‹
            trainerï¼ˆBaseTrainerï¼‰ï¼šç”¨äºŽè®­ç»ƒæ¨¡åž‹çš„è®­ç»ƒå™¨å¯¹è±¡
            ckptï¼ˆdictï¼‰ï¼šå¦‚æžœä»Ž *.pt æ–‡ä»¶åŠ è½½æ¨¡åž‹ï¼Œåˆ™ä¸ºæ£€æŸ¥ç‚¹æ•°æ®
            cfgï¼ˆstrï¼‰ï¼šå¦‚æžœä»Ž *.yaml æ–‡ä»¶åŠ è½½æ¨¡åž‹ï¼Œåˆ™ä¸ºæ¨¡åž‹é…ç½®
            ckpt_pathï¼ˆstrï¼‰ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„
            overridesï¼ˆdictï¼‰ï¼šæ¨¡åž‹é…ç½®çš„è¦†ç›–å­—å…¸
            metricsï¼ˆdictï¼‰ï¼šæœ€æ–°çš„è®­ç»ƒ/éªŒè¯æŒ‡æ ‡
            sessionï¼ˆHUBTrainingSessionï¼‰ï¼šå¦‚æžœé€‚ç”¨ï¼Œä¸º Ultralytics HUB ä¼šè¯
            taskï¼ˆstrï¼‰ï¼šæ¨¡åž‹çš„ä»»åŠ¡ç±»åž‹
            model_nameï¼ˆstrï¼‰ï¼šæ¨¡åž‹çš„åç§°

        æ–¹æ³•ï¼š
            callï¼šé¢„æµ‹æ–¹æ³•çš„åˆ«åï¼Œä½¿æ¨¡åž‹å®žä¾‹å¯è°ƒç”¨
            _newï¼šåŸºäºŽé…ç½®æ–‡ä»¶åˆå§‹åŒ–æ–°æ¨¡åž‹
            _loadï¼šä»Žæ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡åž‹
            _check_is_pytorch_modelï¼šç¡®ä¿æ¨¡åž‹æ˜¯ PyTorch æ¨¡åž‹
            reset_weightsï¼šå°†æ¨¡åž‹çš„æƒé‡é‡ç½®ä¸ºåˆå§‹çŠ¶æ€
            loadï¼šä»ŽæŒ‡å®šæ–‡ä»¶åŠ è½½æ¨¡åž‹æƒé‡
            saveï¼šå°†æ¨¡åž‹çš„å½“å‰çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶
            infoï¼šè®°å½•æˆ–è¿”å›žæœ‰å…³æ¨¡åž‹çš„ä¿¡æ¯
            fuseï¼šèžåˆ Conv2d å’Œ BatchNorm2d å±‚ä»¥ä¼˜åŒ–æŽ¨ç†
            predictï¼šæ‰§è¡Œç›®æ ‡æ£€æµ‹é¢„æµ‹
            trackï¼šæ‰§è¡Œç›®æ ‡è·Ÿè¸ª
            valï¼šåœ¨æ•°æ®é›†ä¸ŠéªŒè¯æ¨¡åž‹
            benchmarkï¼šå¯¹å„ç§å¯¼å‡ºæ ¼å¼è¿›è¡ŒåŸºå‡†æµ‹è¯•
            exportï¼šå°†æ¨¡åž‹å¯¼å‡ºä¸ºä¸åŒæ ¼å¼
            trainï¼šåœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡åž‹
            tuneï¼šæ‰§è¡Œè¶…å‚æ•°è°ƒä¼˜
            _applyï¼šå¯¹æ¨¡åž‹çš„å¼ é‡åº”ç”¨å‡½æ•°
            add_callbackï¼šä¸ºäº‹ä»¶æ·»åŠ å›žè°ƒå‡½æ•°
            clear_callbackï¼šæ¸…é™¤äº‹ä»¶çš„æ‰€æœ‰å›žè°ƒ
            reset_callbacksï¼šå°†æ‰€æœ‰å›žè°ƒé‡ç½®ä¸ºé»˜è®¤å‡½æ•°
            is_triton_modelï¼šæ£€æŸ¥æ¨¡åž‹æ˜¯å¦ä¸º Triton Server æ¨¡åž‹
            is_hub_modelï¼šæ£€æŸ¥æ¨¡åž‹æ˜¯å¦ä¸º Ultralytics HUB æ¨¡åž‹
            _reset_ckpt_argsï¼šåŠ è½½ PyTorch æ¨¡åž‹æ—¶é‡ç½®æ£€æŸ¥ç‚¹å‚æ•°
            _smart_loadï¼šæ ¹æ®æ¨¡åž‹ä»»åŠ¡åŠ è½½é€‚å½“çš„æ¨¡å—
            task_mapï¼šæä¾›ä»Žæ¨¡åž‹ä»»åŠ¡åˆ°ç›¸åº”ç±»çš„æ˜ å°„

        å¼‚å¸¸ï¼š
            FileNotFoundErrorï¼šå¦‚æžœæŒ‡å®šçš„æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸å¯è®¿é—®
            ValueErrorï¼šå¦‚æžœæ¨¡åž‹æ–‡ä»¶æˆ–é…ç½®æ— æ•ˆæˆ–ä¸å—æ”¯æŒ
            ImportErrorï¼šå¦‚æžœæœªå®‰è£…ç‰¹å®šæ¨¡åž‹ç±»åž‹æ‰€éœ€çš„ä¾èµ–é¡¹ï¼ˆå¦‚ HUB SDKï¼‰
            TypeErrorï¼šå¦‚æžœæ¨¡åž‹ä¸æ˜¯æ‰€éœ€çš„ PyTorch æ¨¡åž‹
            AttributeErrorï¼šå¦‚æžœæ‰€éœ€çš„å±žæ€§æˆ–æ–¹æ³•æœªå®žçŽ°æˆ–ä¸å¯ç”¨
            NotImplementedErrorï¼šå¦‚æžœä¸æ”¯æŒç‰¹å®šçš„æ¨¡åž‹ä»»åŠ¡æˆ–æ¨¡å¼
    """

    def __init__(
        self,
        model: Union[str, Path] = "yolov8n.pt",
        task: str = None,
        verbose: bool = False,
    ) -> None:
        """
        åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„Modelç±»å®žä¾‹ã€‚

        è¿™ä¸ªæž„é€ å‡½æ•°æ ¹æ®æä¾›çš„æ¨¡åž‹è·¯å¾„æˆ–åç§°æ¥è®¾ç½®æ¨¡åž‹ã€‚å®ƒå¤„ç†å„ç§ç±»åž‹çš„æ¨¡åž‹æ¥æºï¼ŒåŒ…æ‹¬æœ¬åœ°æ–‡ä»¶ã€Ultralytics HUBæ¨¡åž‹å’ŒTriton Serveræ¨¡åž‹ã€‚è¯¥æ–¹æ³•åˆå§‹åŒ–æ¨¡åž‹çš„å‡ ä¸ªé‡è¦å±žæ€§ï¼Œå¹¶ä¸ºè®­ç»ƒã€é¢„æµ‹æˆ–å¯¼å‡ºç­‰æ“ä½œåšå¥½å‡†å¤‡ã€‚

        å‚æ•°ï¼š

        model (Union[str, Path], optional): è¦åŠ è½½æˆ–åˆ›å»ºçš„æ¨¡åž‹æ–‡ä»¶çš„è·¯å¾„æˆ–åç§°ã€‚è¿™å¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€æ¥è‡ªUltralytics HUBçš„æ¨¡åž‹åç§°æˆ–Triton Serveræ¨¡åž‹ã€‚é»˜è®¤ä¸º 'yolov8n.pt'ã€‚
        task (Any, optional): ä¸ŽYOLOæ¨¡åž‹ç›¸å…³çš„ä»»åŠ¡ç±»åž‹ï¼ŒæŒ‡å®šå…¶åº”ç”¨é¢†åŸŸã€‚é»˜è®¤ä¸ºNoneã€‚
        verbose (bool, optional): å¦‚æžœä¸ºTrueï¼Œåˆ™åœ¨æ¨¡åž‹åˆå§‹åŒ–å’ŒåŽç»­æ“ä½œæœŸé—´å¯ç”¨è¯¦ç»†è¾“å‡ºã€‚é»˜è®¤ä¸ºFalseã€‚
        å¼‚å¸¸ï¼š

        FileNotFoundError: å¦‚æžœæŒ‡å®šçš„æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚
        ValueError: å¦‚æžœæ¨¡åž‹æ–‡ä»¶æˆ–é…ç½®æ— æ•ˆæˆ–ä¸å—æ”¯æŒã€‚
        ImportError: å¦‚æžœç‰¹å®šæ¨¡åž‹ç±»åž‹ï¼ˆå¦‚HUB SDKï¼‰çš„å¿…éœ€ä¾èµ–é¡¹æœªå®‰è£…ã€‚
        """
        super().__init__()
        self.callbacks = callbacks.get_default_callbacks()
        self.predictor = None  # reuse predictor
        self.model = None  # model object
        self.trainer = None  # trainer object
        self.ckpt = None  # if loaded from *.pt
        self.cfg = None  # if loaded from *.yaml
        self.ckpt_path = None
        self.overrides = {}  # overrides for trainer object
        self.metrics = None  # validation/training metrics
        self.session = None  # HUB session
        self.task = task  # task type
        model = str(model).strip()  # .strip()æ–¹æ³•ç”¨äºŽè‡ªåŠ¨ç§»é™¤strä¸¤ç«¯å­—ç¬¦ï¼Œé»˜è®¤æ˜¯ç§»é™¤ç©ºæ ¼å­—ç¬¦

        # Check if Ultralytics HUB model from https://hub.ultralytics.com
        if self.is_hub_model(model):
            # Fetch model from HUB
            checks.check_requirements("hub-sdk>=0.0.8")
            self.session = HUBTrainingSession.create_session(model)
            model = self.session.model_file

        # Check if Triton Server model
        elif self.is_triton_model(model):
            self.model_name = self.model = model
            return

        # Load or create new YOLO model
        if Path(model).suffix in {".yaml", ".yml"}:
            # åŸºäºŽyamlæ–°å»ºæ¨¡åž‹
            self._new(model, task=task, verbose=verbose)
        else:
            # åŠ è½½ptæ¨¡åž‹
            self._load(model, task=task)

    def __call__(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        **kwargs,
    ) -> list:
        """
        An alias for the predict method, enabling the model instance to be callable.

        This method simplifies the process of making predictions by allowing the model instance to be called directly
        with the required arguments for prediction.

        Args:
            source (str | Path | int | PIL.Image | np.ndarray, optional): The source of the image for making
                predictions. Accepts various types, including file paths, URLs, PIL images, and numpy arrays.
                Defaults to None.
            stream (bool, optional): If True, treats the input source as a continuous stream for predictions.
                Defaults to False.
            **kwargs (any): Additional keyword arguments for configuring the prediction process.

        Returns:
            (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.
        """
        return self.predict(source, stream, **kwargs)

    @staticmethod
    def is_triton_model(model: str) -> bool:
        """Is model a Triton Server URL string, i.e. <scheme>://<netloc>/<endpoint>/<task_name>"""
        from urllib.parse import urlsplit

        url = urlsplit(model)
        return url.netloc and url.path and url.scheme in {"http", "grpc"}

    @staticmethod
    def is_hub_model(model: str) -> bool:
        """Check if the provided model is a HUB model."""
        return any(
            (
                model.startswith(f"{HUB_WEB_ROOT}/models/"),  # i.e. https://hub.ultralytics.com/models/MODEL_ID
                [len(x) for x in model.split("_")] == [42, 20],  # APIKEY_MODEL
                len(model) == 20 and not Path(model).exists() and all(x not in model for x in "./\\"),  # MODEL
            )
        )

    def _new(self, cfg: str, task=None, model=None, verbose=False) -> None:
        """
        Initializes a new model and infers the task type from the model definitions.

        Args:
            cfg (str): model configuration file
            task (str | None): model task
            model (BaseModel): Customized model.
            verbose (bool): display model info on load
        """
        cfg_dict = yaml_model_load(cfg)
        self.cfg = cfg
        self.task = task or guess_model_task(cfg_dict)
        self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
        self.overrides["model"] = self.cfg
        self.overrides["task"] = self.task

        # Below added to allow export from YAMLs
        self.model.args = {**DEFAULT_CFG_DICT, **self.overrides}  # combine default and model args (prefer model args)
        self.model.task = self.task
        self.model_name = cfg

    def _load(self, weights: str, task=None) -> None:
        """
        åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡åž‹å¹¶ä»Žæ¨¡åž‹çš„headæŽ¨æ–­ä»»åŠ¡ç±»åž‹ã€‚

        å‚æ•°ï¼š

        weights (str): è¦åŠ è½½çš„æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚
        task (str | None): æ¨¡åž‹ä»»åŠ¡ã€‚
        """
        if weights.lower().startswith(("https://", "http://", "rtsp://", "rtmp://", "tcp://")):
            weights = checks.check_file(weights)  # automatically download and return local filename
        weights = checks.check_model_file_from_stem(weights)  # add suffix, i.e. yolov8n -> yolov8n.pt

        if Path(weights).suffix == ".pt":
            self.model, self.ckpt = attempt_load_one_weight(weights)  # åŸºäºŽtorch.load()åŠ è½½æ¨¡åž‹
            self.task = self.model.args["task"]
            self.overrides = self.model.args = self._reset_ckpt_args(self.model.args)  # æå–"imgsz", "data", "task", "single_cls"ï¼Œonly remember these arguments when loading a PyTorch model
            self.ckpt_path = self.model.pt_path
        else:
            weights = checks.check_file(weights)  # runs in all cases, not redundant with above call
            self.model, self.ckpt = weights, None
            self.task = task or guess_model_task(weights)
            self.ckpt_path = weights
        self.overrides["model"] = weights
        self.overrides["task"] = self.task
        self.model_name = weights

    def _check_is_pytorch_model(self) -> None:
        """Raises TypeError is model is not a PyTorch model."""
        pt_str = isinstance(self.model, (str, Path)) and Path(self.model).suffix == ".pt"  # æ£€æŸ¥åŽç¼€
        pt_module = isinstance(self.model, nn.Module)  # æ£€æŸ¥modelæ˜¯å¦æ˜¯nn.Moduleç±»å¯¹è±¡
        if not (pt_module or pt_str):
            raise TypeError(
                f"model='{self.model}' should be a *.pt PyTorch model to run this method, but is a different format. "
                f"PyTorch models can train, val, predict and export, i.e. 'model.train(data=...)', but exported "
                f"formats like ONNX, TensorRT etc. only support 'predict' and 'val' modes, "
                f"i.e. 'yolo predict model=yolov8n.onnx'.\nTo run CUDA or MPS inference please pass the device "
                f"argument directly in your inference command, i.e. 'model.predict(source=..., device=0)'"
            )

    def reset_weights(self) -> "Model":
        """
        Resets the model parameters to randomly initialized values, effectively discarding all training information.

        This method iterates through all modules in the model and resets their parameters if they have a
        'reset_parameters' method. It also ensures that all parameters have 'requires_grad' set to True, enabling them
        to be updated during training.

        Returns:
            self (ultralytics.engine.model.Model): The instance of the class with reset weights.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        for m in self.model.modules():
            if hasattr(m, "reset_parameters"):
                m.reset_parameters()
        for p in self.model.parameters():
            p.requires_grad = True
        return self

    def load(self, weights: Union[str, Path] = "yolov8n.pt") -> "Model":
        """
        Loads parameters from the specified weights file into the model.

        This method supports loading weights from a file or directly from a weights object. It matches parameters by
        name and shape and transfers them to the model.

        Args:
            weights (str | Path): Path to the weights file or a weights object. Defaults to 'yolov8n.pt'.

        Returns:
            self (ultralytics.engine.model.Model): The instance of the class with loaded weights.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        if isinstance(weights, (str, Path)):
            weights, self.ckpt = attempt_load_one_weight(weights)
        self.model.load(weights)
        return self

    def save(self, filename: Union[str, Path] = "saved_model.pt", use_dill=True) -> None:
        """
        Saves the current model state to a file.

        This method exports the model's checkpoint (ckpt) to the specified filename.

        Args:
            filename (str | Path): The name of the file to save the model to. Defaults to 'saved_model.pt'.
            use_dill (bool): Whether to try using dill for serialization if available. Defaults to True.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        from copy import deepcopy
        from datetime import datetime

        from ultralytics import __version__

        updates = {
            "model": deepcopy(self.model).half() if isinstance(self.model, nn.Module) else self.model,
            "date": datetime.now().isoformat(),
            "version": __version__,
            "license": "AGPL-3.0 License (https://ultralytics.com/license)",
            "docs": "https://docs.ultralytics.com",
        }
        torch.save({**self.ckpt, **updates}, filename, use_dill=use_dill)

    def info(self, detailed: bool = False, verbose: bool = True):
        """
        Logs or returns model information.

        This method provides an overview or detailed information about the model, depending on the arguments passed.
        It can control the verbosity of the output.

        Args:
            detailed (bool): If True, shows detailed information about the model. Defaults to False.
            verbose (bool): If True, prints the information. If False, returns the information. Defaults to True.

        Returns:
            (list): Various types of information about the model, depending on the 'detailed' and 'verbose' parameters.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        return self.model.info(detailed=detailed, verbose=verbose)

    def fuse(self):
        """
        Fuses Conv2d and BatchNorm2d layers in the model.

        This method optimizes the model by fusing Conv2d and BatchNorm2d layers, which can improve inference speed.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        self.model.fuse()

    def embed(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        **kwargs,
    ) -> list:
        """
        Generates image embeddings based on the provided source.

        This method is a wrapper around the 'predict()' method, focusing on generating embeddings from an image source.
        It allows customization of the embedding process through various keyword arguments.

        Args:
            source (str | int | PIL.Image | np.ndarray): The source of the image for generating embeddings.
                The source can be a file path, URL, PIL image, numpy array, etc. Defaults to None.
            stream (bool): If True, predictions are streamed. Defaults to False.
            **kwargs (any): Additional keyword arguments for configuring the embedding process.

        Returns:
            (List[torch.Tensor]): A list containing the image embeddings.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        if not kwargs.get("embed"):
            kwargs["embed"] = [len(self.model.model) - 2]  # embed second-to-last layer if no indices passed
        return self.predict(source, stream, **kwargs)

    def predict(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        predictor=None,
        **kwargs,
    ) -> List[Results]:
        """
        å¯¹ç»™å®šçš„å›¾åƒæºä½¿ç”¨ YOLO æ¨¡åž‹è¿›è¡Œé¢„æµ‹ã€‚

        è¿™ä¸ªæ–¹æ³•ç®€åŒ–äº†é¢„æµ‹è¿‡ç¨‹ï¼Œå…è®¸é€šè¿‡å…³é”®å­—å‚æ•°è¿›è¡Œå„ç§é…ç½®ã€‚å®ƒæ”¯æŒä½¿ç”¨è‡ªå®šä¹‰é¢„æµ‹å™¨æˆ–é»˜è®¤é¢„æµ‹å™¨æ–¹æ³•è¿›è¡Œé¢„æµ‹ã€‚è¯¥æ–¹æ³•å¤„ç†ä¸åŒç±»åž‹çš„å›¾åƒæºï¼Œå¹¶ä¸”å¯ä»¥ä»¥æµæ¨¡å¼è¿è¡Œã€‚å®ƒè¿˜é€šè¿‡'prompts'æ”¯æŒSAMç±»åž‹æ¨¡åž‹ã€‚

        å¦‚æžœå°šæœªå­˜åœ¨é¢„æµ‹å™¨ï¼Œè¯¥æ–¹æ³•ä¼šè®¾ç½®ä¸€ä¸ªæ–°çš„é¢„æµ‹å™¨ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ›´æ–°å…¶å‚æ•°ã€‚å¦‚æžœæœªæä¾›'source'ï¼Œè¯¥æ–¹æ³•ä¼šå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨é»˜è®¤èµ„æºã€‚è¯¥æ–¹æ³•ç¡®å®šæ˜¯å¦ä»Žå‘½ä»¤è¡Œç•Œé¢è°ƒç”¨ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´å…¶è¡Œä¸ºï¼ŒåŒ…æ‹¬è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼å’Œä¿å­˜è¡Œä¸ºçš„é»˜è®¤å€¼ã€‚

        å‚æ•°ï¼š

        source (str | int | PIL.Image | np.ndarray, optional): ç”¨äºŽè¿›è¡Œé¢„æµ‹çš„å›¾åƒæºã€‚æŽ¥å—å„ç§ç±»åž‹ï¼ŒåŒ…æ‹¬æ–‡ä»¶è·¯å¾„ã€URLã€PILå›¾åƒå’Œnumpyæ•°ç»„ã€‚é»˜è®¤ä¸ºASSETSã€‚
        stream (bool, optional): å°†è¾“å…¥æºè§†ä¸ºè¿žç»­æµè¿›è¡Œé¢„æµ‹ã€‚é»˜è®¤ä¸ºFalseã€‚
        predictor (BasePredictor, optional): ç”¨äºŽè¿›è¡Œé¢„æµ‹çš„è‡ªå®šä¹‰é¢„æµ‹å™¨ç±»çš„å®žä¾‹ã€‚å¦‚æžœä¸ºNoneï¼Œæ–¹æ³•ä½¿ç”¨é»˜è®¤é¢„æµ‹å™¨ã€‚é»˜è®¤ä¸ºNoneã€‚
        **kwargs (any): é…ç½®é¢„æµ‹è¿‡ç¨‹çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚è¿™äº›å‚æ•°å…è®¸è¿›ä¸€æ­¥è‡ªå®šä¹‰é¢„æµ‹è¡Œä¸ºã€‚
        è¿”å›žï¼š

        (List[ultralytics.engine.results.Results]): ä¸€ä¸ªé¢„æµ‹ç»“æžœåˆ—è¡¨ï¼Œå°è£…åœ¨Resultsç±»ä¸­ã€‚
        å¼•å‘ï¼š

        AttributeError: å¦‚æžœé¢„æµ‹å™¨æœªæ­£ç¡®è®¾ç½®ã€‚
        """
        if source is None:
            source = ASSETS
            LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")

        is_cli = (ARGV[0].endswith("yolo") or ARGV[0].endswith("ultralytics")) and any(
            x in ARGV for x in ("predict", "track", "mode=predict", "mode=track")
        )

        custom = {"conf": 0.25, "batch": 1, "save": is_cli, "mode": "predict"}  # method defaults
        args = {**self.overrides, **custom, **kwargs}  # highest priority args on the right
        prompts = args.pop("prompts", None)  # for SAM-type models

        if not self.predictor:
            self.predictor = predictor or self._smart_load("predictor")(overrides=args, _callbacks=self.callbacks)
            self.predictor.setup_model(model=self.model, verbose=is_cli)
        else:  # only update args if predictor is already setup
            self.predictor.args = get_cfg(self.predictor.args, args)
            if "project" in args or "name" in args:
                self.predictor.save_dir = get_save_dir(self.predictor.args)
        if prompts and hasattr(self.predictor, "set_prompts"):  # for SAM-type models
            self.predictor.set_prompts(prompts)
        return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)

    def track(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        persist: bool = False,
        **kwargs,
    ) -> List[Results]:
        """
        Conducts object tracking on the specified input source using the registered trackers.

        This method performs object tracking using the model's predictors and optionally registered trackers. It is
        capable of handling different types of input sources such as file paths or video streams. The method supports
        customization of the tracking process through various keyword arguments. It registers trackers if they are not
        already present and optionally persists them based on the 'persist' flag.

        The method sets a default confidence threshold specifically for ByteTrack-based tracking, which requires low
        confidence predictions as input. The tracking mode is explicitly set in the keyword arguments.

        Args:
            source (str, optional): The input source for object tracking. It can be a file path, URL, or video stream.
            stream (bool, optional): Treats the input source as a continuous video stream. Defaults to False.
            persist (bool, optional): Persists the trackers between different calls to this method. Defaults to False.
            **kwargs (any): Additional keyword arguments for configuring the tracking process. These arguments allow
                for further customization of the tracking behavior.

        Returns:
            (List[ultralytics.engine.results.Results]): A list of tracking results, encapsulated in the Results class.

        Raises:
            AttributeError: If the predictor does not have registered trackers.
        """
        if not hasattr(self.predictor, "trackers"):
            from ultralytics.trackers import register_tracker

            register_tracker(self, persist)
        kwargs["conf"] = kwargs.get("conf") or 0.1  # ByteTrack-based method needs low confidence predictions as input
        kwargs["batch"] = kwargs.get("batch") or 1  # batch-size 1 for tracking in videos
        kwargs["mode"] = "track"
        return self.predict(source=source, stream=stream, **kwargs)

    def val(
        self,
        validator=None,
        **kwargs,
    ):
        """
        Validates the model using a specified dataset and validation configuration.

        This method facilitates the model validation process, allowing for a range of customization through various
        settings and configurations. It supports validation with a custom validator or the default validation approach.
        The method combines default configurations, method-specific defaults, and user-provided arguments to configure
        the validation process. After validation, it updates the model's metrics with the results obtained from the
        validator.

        The method supports various arguments that allow customization of the validation process. For a comprehensive
        list of all configurable options, users should refer to the 'configuration' section in the documentation.

        Args:
            validator (BaseValidator, optional): An instance of a custom validator class for validating the model. If
                None, the method uses a default validator. Defaults to None.
            **kwargs (any): Arbitrary keyword arguments representing the validation configuration. These arguments are
                used to customize various aspects of the validation process.

        Returns:
            (ultralytics.utils.metrics.DetMetrics): Validation metrics obtained from the validation process.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        custom = {"rect": True}  # method defaults
        args = {**self.overrides, **custom, **kwargs, "mode": "val"}  # highest priority args on the right

        validator = (validator or self._smart_load("validator"))(args=args, _callbacks=self.callbacks)
        validator(model=self.model)
        self.metrics = validator.metrics
        return validator.metrics

    def benchmark(
        self,
        **kwargs,
    ):
        """
        Benchmarks the model across various export formats to evaluate performance.

        This method assesses the model's performance in different export formats, such as ONNX, TorchScript, etc.
        It uses the 'benchmark' function from the ultralytics.utils.benchmarks module. The benchmarking is configured
        using a combination of default configuration values, model-specific arguments, method-specific defaults, and
        any additional user-provided keyword arguments.

        The method supports various arguments that allow customization of the benchmarking process, such as dataset
        choice, image size, precision modes, device selection, and verbosity. For a comprehensive list of all
        configurable options, users should refer to the 'configuration' section in the documentation.

        Args:
            **kwargs (any): Arbitrary keyword arguments to customize the benchmarking process. These are combined with
                default configurations, model-specific arguments, and method defaults.

        Returns:
            (dict): A dictionary containing the results of the benchmarking process.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        from ultralytics.utils.benchmarks import benchmark

        custom = {"verbose": False}  # method defaults
        args = {**DEFAULT_CFG_DICT, **self.model.args, **custom, **kwargs, "mode": "benchmark"}
        return benchmark(
            model=self,
            data=kwargs.get("data"),  # if no 'data' argument passed set data=None for default datasets
            imgsz=args["imgsz"],
            half=args["half"],
            int8=args["int8"],
            device=args["device"],
            verbose=kwargs.get("verbose"),
        )

    def export(
        self,
        **kwargs,
    ) -> str:
        """
        Exports the model to a different format suitable for deployment.

        This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment
        purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method
        defaults, and any additional arguments provided. The combined arguments are used to configure export settings.

        The method supports a wide range of arguments to customize the export process. For a comprehensive list of all
        possible arguments, refer to the 'configuration' section in the documentation.

        Args:
            **kwargs (any): Arbitrary keyword arguments to customize the export process. These are combined with the
                model's overrides and method defaults.

        Returns:
            (str): The exported model filename in the specified format, or an object related to the export process.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        from .exporter import Exporter

        custom = {
            "imgsz": self.model.args["imgsz"],
            "batch": 1,
            "data": None,
            "device": None,  # reset to avoid multi-GPU errors
            "verbose": False,
        }  # method defaults
        args = {**self.overrides, **custom, **kwargs, "mode": "export"}  # highest priority args on the right
        return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)

    def train(
        self,
        trainer=None,
        **kwargs,  # {'batch': 4, 'data': './pipe.yaml', 'epochs': 300, 'imgsz': 1500, 'max_det': 2000, 'workers': 1}
    ):
        """
        ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†å’Œè®­ç»ƒé…ç½®è®­ç»ƒæ¨¡åž‹ã€‚

        æ­¤æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—å¯å®šåˆ¶çš„è®¾ç½®å’Œé…ç½®æ¥ä¿ƒè¿›æ¨¡åž‹è®­ç»ƒã€‚å®ƒæ”¯æŒä½¿ç”¨è‡ªå®šä¹‰çš„è®­ç»ƒå™¨æˆ–æ–¹æ³•ä¸­å®šä¹‰çš„é»˜è®¤è®­ç»ƒæ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚
        è¯¥æ–¹æ³•å¤„ç†ä¸åŒçš„æƒ…å†µï¼Œä¾‹å¦‚ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€ä¸Ž Ultralytics HUB é›†æˆä»¥åŠåœ¨è®­ç»ƒåŽæ›´æ–°æ¨¡åž‹å’Œé…ç½®ã€‚
        å½“ä½¿ç”¨ Ultralytics HUB æ—¶ï¼Œå¦‚æžœä¼šè¯å·²ç»åŠ è½½äº†æ¨¡åž‹ï¼Œåˆ™æ–¹æ³•ä¼˜å…ˆè€ƒè™‘ HUB çš„è®­ç»ƒå‚æ•°ï¼Œå¹¶åœ¨æä¾›æœ¬åœ°å‚æ•°æ—¶å‘å‡ºè­¦å‘Šã€‚
        å®ƒæ£€æŸ¥ pip æ›´æ–°ï¼Œå¹¶ç»“åˆé»˜è®¤é…ç½®ã€æ–¹æ³•ç‰¹å®šçš„é»˜è®¤å€¼å’Œç”¨æˆ·æä¾›çš„å‚æ•°æ¥é…ç½®è®­ç»ƒè¿‡ç¨‹ã€‚è®­ç»ƒåŽï¼Œå®ƒæ›´æ–°æ¨¡åž‹åŠå…¶é…ç½®ï¼Œå¹¶å¯é€‰æ‹©æ€§åœ°é™„åŠ åº¦é‡æŒ‡æ ‡ã€‚

        å‚æ•°ï¼š
            trainer (BaseTrainer, optional): è‡ªå®šä¹‰è®­ç»ƒå™¨ç±»çš„å®žä¾‹ï¼Œç”¨äºŽè®­ç»ƒæ¨¡åž‹ã€‚å¦‚æžœä¸º Noneï¼Œåˆ™æ–¹æ³•ä½¿ç”¨é»˜è®¤è®­ç»ƒå™¨ã€‚é»˜è®¤å€¼ä¸º Noneã€‚
            **kwargs (any): è¡¨ç¤ºè®­ç»ƒé…ç½®çš„ä»»æ„å…³é”®å­—å‚æ•°ã€‚è¿™äº›å‚æ•°ç”¨äºŽå®šåˆ¶è®­ç»ƒè¿‡ç¨‹çš„å„ä¸ªæ–¹é¢ã€‚
        è¿”å›žå€¼ï¼š
            (dict | None): å¦‚æžœå¯ç”¨ä¸”è®­ç»ƒæˆåŠŸï¼Œåˆ™è¿”å›žè®­ç»ƒæŒ‡æ ‡ï¼›å¦åˆ™è¿”å›ž Noneã€‚
        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æžœæ¨¡åž‹ä¸æ˜¯ PyTorch æ¨¡åž‹ã€‚
            PermissionError: å¦‚æžœ HUB ä¼šè¯å­˜åœ¨æƒé™é—®é¢˜ã€‚
            ModuleNotFoundError: å¦‚æžœæœªå®‰è£… HUB SDKã€‚
        """
        self._check_is_pytorch_model()
        if hasattr(self.session, "model") and self.session.model.id:  # Ultralytics HUB session with loaded model
            if any(kwargs):
                LOGGER.warning("WARNING âš ï¸ using HUB training arguments, ignoring local training arguments.")
            kwargs = self.session.train_args  # overwrite kwargs

        checks.check_pip_update_available()

        overrides = yaml_load(checks.check_yaml(kwargs["cfg"])) if kwargs.get("cfg") else self.overrides  # {'data': 'coco.yaml', 'imgsz': 640, 'model': './yolov8m.pt', 'single_cls': False, 'task': 'detect'}
        custom = {
            # NOTE: handle the case when 'cfg' includes 'data'.
            "data": overrides.get("data") or DEFAULT_CFG_DICT["data"] or TASK2DATA[self.task],
            "model": self.overrides["model"],
            "task": self.task,
        }  # method defaults
        args = {**overrides, **custom, **kwargs, "mode": "train"}  # åˆå¹¶å¤šä¸ªå‚æ•°å­—å…¸æ—¶ï¼Œæœ€å³è¾¹çš„å­—å…¸å…·æœ‰æœ€é«˜ä¼˜å…ˆçº§ã€‚
        if args.get("resume"):
            args["resume"] = self.ckpt_path

        self.trainer = (trainer or self._smart_load("trainer"))(overrides=args, _callbacks=self.callbacks)  # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œå³åˆ›å»º ultralytics.models.yolo.detect.train.DetectionTrainer ç±»çš„å¯¹è±¡ä½œä¸ºtrainerã€‚æ­¤å¤„ä»…è¯»å–æ•°æ®é›†åœ°å€åŠå…¶ä»–å‚æ•°
        if not args.get("resume"):  # å¦‚æžœä¸æ˜¯resumeï¼Œæ‰‹åŠ¨å°†æ¨¡åž‹å¯¹è±¡é€å…¥trainer
            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)
            self.model = self.trainer.model

        self.trainer.hub_session = self.session  # attach optional HUB session
        self.trainer.train()
        # Update model and cfg after training
        if RANK in {-1, 0}:
            ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last
            self.model, _ = attempt_load_one_weight(ckpt)
            self.overrides = self.model.args
            self.metrics = getattr(self.trainer.validator, "metrics", None)  # TODO: no metrics returned by DDP
        return self.metrics

    def tune(
        self,
        use_ray=False,
        iterations=10,
        *args,
        **kwargs,
    ):
        """
        Conducts hyperparameter tuning for the model, with an option to use Ray Tune.

        This method supports two modes of hyperparameter tuning: using Ray Tune or a custom tuning method.
        When Ray Tune is enabled, it leverages the 'run_ray_tune' function from the ultralytics.utils.tuner module.
        Otherwise, it uses the internal 'Tuner' class for tuning. The method combines default, overridden, and
        custom arguments to configure the tuning process.

        Args:
            use_ray (bool): If True, uses Ray Tune for hyperparameter tuning. Defaults to False.
            iterations (int): The number of tuning iterations to perform. Defaults to 10.
            *args (list): Variable length argument list for additional arguments.
            **kwargs (any): Arbitrary keyword arguments. These are combined with the model's overrides and defaults.

        Returns:
            (dict): A dictionary containing the results of the hyperparameter search.

        Raises:
            AssertionError: If the model is not a PyTorch model.
        """
        self._check_is_pytorch_model()
        if use_ray:
            from ultralytics.utils.tuner import run_ray_tune

            return run_ray_tune(self, max_samples=iterations, *args, **kwargs)
        else:
            from .tuner import Tuner

            custom = {}  # method defaults
            args = {**self.overrides, **custom, **kwargs, "mode": "train"}  # highest priority args on the right
            return Tuner(args=args, _callbacks=self.callbacks)(model=self, iterations=iterations)

    def _apply(self, fn) -> "Model":
        """Apply to(), cpu(), cuda(), half(), float() to model tensors that are not parameters or registered buffers."""
        self._check_is_pytorch_model()
        self = super()._apply(fn)  # noqa
        self.predictor = None  # reset predictor as device may have changed
        self.overrides["device"] = self.device  # was str(self.device) i.e. device(type='cuda', index=0) -> 'cuda:0'
        return self

    @property
    def names(self) -> list:
        """
        Retrieves the class names associated with the loaded model.

        This property returns the class names if they are defined in the model. It checks the class names for validity
        using the 'check_class_names' function from the ultralytics.nn.autobackend module.

        Returns:
            (list | None): The class names of the model if available, otherwise None.
        """
        from ultralytics.nn.autobackend import check_class_names

        if hasattr(self.model, "names"):
            return check_class_names(self.model.names)
        if not self.predictor:  # export formats will not have predictor defined until predict() is called
            self.predictor = self._smart_load("predictor")(overrides=self.overrides, _callbacks=self.callbacks)
            self.predictor.setup_model(model=self.model, verbose=False)
        return self.predictor.model.names

    @property
    def device(self) -> torch.device:
        """
        Retrieves the device on which the model's parameters are allocated.

        This property is used to determine whether the model's parameters are on CPU or GPU. It only applies to models
        that are instances of nn.Module.

        Returns:
            (torch.device | None): The device (CPU/GPU) of the model if it is a PyTorch model, otherwise None.
        """
        return next(self.model.parameters()).device if isinstance(self.model, nn.Module) else None

    @property
    def transforms(self):
        """
        Retrieves the transformations applied to the input data of the loaded model.

        This property returns the transformations if they are defined in the model.

        Returns:
            (object | None): The transform object of the model if available, otherwise None.
        """
        return self.model.transforms if hasattr(self.model, "transforms") else None

    def add_callback(self, event: str, func) -> None:
        """
        Adds a callback function for a specified event.

        This method allows the user to register a custom callback function that is triggered on a specific event during
        model training or inference.

        Args:
            event (str): The name of the event to attach the callback to.
            func (callable): The callback function to be registered.

        Raises:
            ValueError: If the event name is not recognized.
        """
        self.callbacks[event].append(func)

    def clear_callback(self, event: str) -> None:
        """
        Clears all callback functions registered for a specified event.

        This method removes all custom and default callback functions associated with the given event.

        Args:
            event (str): The name of the event for which to clear the callbacks.

        Raises:
            ValueError: If the event name is not recognized.
        """
        self.callbacks[event] = []

    def reset_callbacks(self) -> None:
        """
        Resets all callbacks to their default functions.

        This method reinstates the default callback functions for all events, removing any custom callbacks that were
        added previously.
        """
        for event in callbacks.default_callbacks.keys():
            self.callbacks[event] = [callbacks.default_callbacks[event][0]]

    @staticmethod
    def _reset_ckpt_args(args: dict) -> dict:
        """Reset arguments when loading a PyTorch model."""
        include = {"imgsz", "data", "task", "single_cls"}  # only remember these arguments when loading a PyTorch model
        return {k: v for k, v in args.items() if k in include}

    # def __getattr__(self, attr):
    #    """Raises error if object has no requested attribute."""
    #    name = self.__class__.__name__
    #    raise AttributeError(f"'{name}' object has no attribute '{attr}'. See valid attributes below.\n{self.__doc__}")

    def _smart_load(self, key: str):
        """Load model/trainer/validator/predictor."""
        try:
            return self.task_map[self.task][key]  # è¿”å›žçš„æ˜¯ ultralytics.models.yolo.detect.train.DetectionTrainer è¿™ä¸ªç±»
        except Exception as e:
            name = self.__class__.__name__
            mode = inspect.stack()[1][3]  # get the function name.
            raise NotImplementedError(
                emojis(f"WARNING âš ï¸ '{name}' model does not support '{mode}' mode for '{self.task}' task yet.")
            ) from e

    @property
    def task_map(self) -> dict:
        """
        Map head to model, trainer, validator, and predictor classes.

        Returns:
            task_map (dict): The map of model task to mode classes.
        """
        raise NotImplementedError("Please provide task map for your model!")
